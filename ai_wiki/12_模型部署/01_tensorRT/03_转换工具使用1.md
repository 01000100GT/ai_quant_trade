# 1. 简介
## 1.1 trexe  
&emsp;&emsp;trexe是一个CLI模型转换及分析的工具，如下是基于8.5版本。
&emsp;&emsp;文档：https://docs.nvidia.cn/deeplearning/tensorrt/developer-guide/index.html#trtexec

### 1.1.1 模型转换
&emsp;&emsp;CLI对应参数如下：     
Flags for the Build Phase   
* --onnx=<model>: Specify the input ONNX model.
* --deploy=<caffe_prototxt>: Specify the input Caffe prototxt model.
* --uff=<model>: Specify the input UFF model.
* --output=<tensor>: Specify output tensor names. Only required if the input models are in UFF or Caffe formats.
* --maxBatch=<BS>: Specify the maximum batch size to build the engine with. Only needed if the input models are in UFF or Caffe formats. If the input model is in ONNX format, use the --minShapes, --optShapes, --maxShapes flags to control the range of input shapes including batch size.
* --minShapes=<shapes>, --optShapes=<shapes>, --maxShapes=<shapes>: Specify the range of the input shapes to build the engine with. Only required if the input model is in ONNX format.
* --workspace=<size in MB>: Specify the maximum size of the workspace that tactics are allowed to use. This flag has been deprecated. You can use the –-memPoolSize=<pool_spec> flag instead.
* –-memPoolSize=<pool_spec>: Specify the maximum size of the workspace that tactics are allowed to use, as well as the sizes of the memory pools that DLA will allocate per loadable.
* --saveEngine=<file>: Specify the path to save the engine to.
* --fp16, --int8, --noTF32, --best: Specify network-level precision.
* --sparsity=[disable|enable|force]: Specify whether to use tactics that support structured sparsity.
    disable: Disable all tactics using structured sparsity. This is the default.
    enable: Enable tactics using structured sparsity. Tactics will only be used if the weights in the ONNX file meet the requirements for structured sparsity.
    force: Enable tactics using structured sparsity and allow trtexec to overwrite the weights in the ONNX file to enforce them to have structured sparsity patterns. Note that the accuracy is not preserved, so this is to get inference performance only.
* --timingCacheFile=<file>: Specify the timing cache to load from and save to.
* --verbose: Turn on verbose logging.
* --buildOnly: Build and save the engine without running inference.
* --profilingVerbosity=[layer_names_only|detailed|none]: Specify the profiling verbosity to build the engine with.
* --dumpLayerInfo, --exportLayerInfo=<file>: Print/Save the layer information of the engine.
* --precisionConstraints=spec: Control precision constraint setting.
    none: No constraints.
    prefer: Meet precision constraints set by --layerPrecisions/--layerOutputTypes if possible.
    obey: Meet precision constraints set by --layerPrecisions/--layerOutputTypes or fail otherwise.
* --layerPrecisions=spec: Control per-layer precision constraints. Effective only when precisionConstraints is set to obey or prefer. The specs are read left to right, and later ones override earlier ones. "*" can be used as a layerName to specify the default precision for all the unspecified layers.
    For example: --layerPrecisions=*:fp16,layer_1:fp32 sets the precision of all layers to FP16 except for layer_1, which will be set to FP32.
* --layerOutputTypes=spec: Control per-layer output type constraints. Effective only when precisionConstraints is set to obey or prefer. The specs are read left to right, and later ones override earlier ones. "*" can be used as a layerName to specify the default precision for all the unspecified layers. If a layer has more than one output, then multiple types separated by "+" can be provided for this layer.
    For example: --layerOutputTypes=*:fp16,layer_1:fp32+fp16 sets the precision of all layer outputs to FP16 except for layer_1, whose first output will be set to FP32 and whose second output will be set to FP16.
* –useDLACore=N: Use the specified DLA core for layers that support DLA.
* –allowGPUFallback: Allow layers unsupported on DLA to run on GPU instead.

Flags for the Inference Phase
* --loadEngine=<file>: Load the engine from a serialized plan file instead of building it from input ONNX, UFF, or Caffe model.
* --batch=<N>: Specify the batch size to run the inference with. Only needed if the input models are in UFF or Caffe formats. If the input model is in ONNX format or if the engine is built with explicit batch dimension, use --shapes instead.
* --shapes=<shapes>: Specify the input shapes to run the inference with.
* --warmUp=<duration in ms>, --duration=<duration in seconds>, --iterations=<N>: Specify the minimum duration of the warm-up runs, the minimum duration for the inference runs, and the minimum iterations of the inference runs. For example, setting --warmUp=0 --duration=0 --iterations allows users to control exactly how many iterations to run the inference for.
* --useCudaGraph: Capture the inference to a CUDA graph and run inference by launching the graph. This argument may be ignored when the built TensorRT engine contains operations that are not permitted under CUDA graph capture mode.
* --noDataTransfers: Turn off host to device and device-to-host data transfers.
* --streams=<N>: Run inference with multiple streams in parallel.
* --verbose: Turn on verbose logging.
* --dumpProfile, --exportProfile=<file>: Print/Save the per-layer performance profile.

Refer to trtexec --help for all the supported flags and detailed explanations.
Refer to GitHub: trtexec/README.md file for detailed information about how to build this tool and examples of its usage.


### 1.1.2 模型优化
1. 数据搬运时间优化
    &emsp;&emsp;如果Throughput或Enqueue Time时间远低于GPU Compute时间，GPU利用率低，可能由于host繁忙或数据搬运。
    Using CUDA graphs (with --useCudaGraph) or disabling H2D/D2H transfers (with --noDataTransfer) 
    may improve GPU utilization.
![](.01_简介及文档资料_images/TensorRT耗时分析图.png)

### 1.1.2 性能分析
* --dumpProfile到处模型每层耗时（需要CUDA 11.1以上版本）。 
* --profilingVerbosity=detailed,查看更详细的引擎信息和数据绑定信息。