{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "uijiWgkuh1jB",
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ]
  },
  "kernelspec": {
   "name": "pycharm-3ea5732f",
   "language": "python",
   "display_name": "PyCharm (ai_quant_trade)"
  },
  "language_info": {
   "name": "python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 安装依赖包，导入头文件\n",
    "\n",
    "本样例复现论文：*the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*.\n",
    "\n",
    "代码参考：[FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))"
   ],
   "metadata": {
    "id": "gT-zXutMgqOS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "原理介绍：\n",
    "\n",
    "强化学习核心部分包括“机器人”和“环境”。流程大致如下：\n",
    "- 机器人和环境进行交互，观察到当前的条件，称为“状态”（**state**），并且可以执行“动作”（**action**）\n",
    "- 机器人执行动作后，会进入一个新的状态，同时，环境给机器人一个反馈，叫奖励（**reward**）\n",
    "  (通过数字反馈新状态的好坏)\n",
    "- 之后，机器人和环境不停的重复交互，机器人要尽可能多的获取累计奖励\n",
    "\n",
    "强化学习是一种方法，让机器人学会提升表现，并达成目标。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "实现介绍\n",
    "\n",
    "使用OpenAI gym的格式构建股票交易的环境。\n",
    "\n",
    "state-action-reward的含义如下：\n",
    "\n",
    "- **State s**: 状态空间表示机器人对环境的感知。就像人工交易员分析各种信息和数据。机器人从历史数据\n",
    "  观察交易价格以及技术指标。通过和环境交互进行学习（一般通过回放历史数据）\n",
    "  \n",
    "- **Action a**: 动作空间代码机器人在每个状态可以执行的动作。例如，a ∈ {−1, 0, 1}, −1, 0, 1代表\n",
    "  卖出、持仓、买入。当处理多支股票时，a ∈{−k, ..., −1, 0, 1, ..., k}, 比如，“买10股AAPL”或者\n",
    "  “卖出10股AAPL”即10或-10。\n",
    "\n",
    "- **Reward function r(s, a, s′)**: 奖励用于激励机器人学习一个更好的策略。例如，在状态s下执行动作a\n",
    "  以改变投资组合值，并到达一个新的状态s', 例如，r(s, a, s′) = v′ − v, v′ 和 v 代表状态分别在s′ \n",
    "  和s时的投资组合总市值。\n",
    "  \n",
    "- **Market environment**: 道琼斯工业平均指数（DJIA）中30只成分股，包含回测时间段的所有交易数据。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 如果没有安装，解注释进行安装\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "## 请把下面解注释，安装finrl库\n",
    "# 或者把如下Github仓中的finrl文件夹考到根目录即可使用\n",
    "##!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
    "\n",
    "# 强化学习库，使用stable_baselines3\n",
    "# 注意：\n",
    "#    1. 强化学习比起机器学习慢很多，CPU训练大约2分钟，强化学习使用单卡GPU大约需要30分钟左右完成训练\n",
    "#    2. 强化学习不稳定，每次收敛的loss不一样，且效果可能差异大     "
   ],
   "metadata": {
    "id": "D0vEcPxSJ8hI",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.meta.data_processor import DataProcessor"
   ],
   "metadata": {
    "id": "xt1317y2ixSS",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from finrl import config  # 包含各类超参\n",
    "from finrl import config_tickers  # 常见各类市场股票代码集合，比如中证300\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "# 创建目录\n",
    "check_and_make_directories([TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ],
   "metadata": {
    "id": "wZ7Bl7i6I2AM",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 在OpenAI Gym-style构建市场环境"
   ],
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 读取训练数据，所有股票均混在了一个csv表里，格式如下\n",
    "# 索引     日期          股票\n",
    "#  0      2009-01-02    苹果\n",
    "#  0      2009-01-02    亚马逊\n",
    "#  1      2009-01-05    苹果\n",
    "#  1      2009-01-05    亚马逊\n",
    "\n",
    "# 注意：必须保持上述该格式，同样的索引下至少有2个数据，否则会报错，\n",
    "# 原因：\n",
    "#   1. 在finrl/meta/env_stock_trading/env_stocktrading.py的\n",
    "#      _initiate_state函数中self.data.close.values.tolist()，\n",
    "#      在404行，要求self.data.close必须是二维数组\n",
    "#   2. 而finrl/meta/env_stock_trading/env_stocktrading.py的\n",
    "#      __init__的64行self.data = self.df.loc[self.day, :]，\n",
    "#      如果索引顺序排，0，1，2。。。，会导致只取到一个行数，一维\n",
    "#      数据传入导致第1点中所述的问题\n",
    "#      （因此，如果只有一支股票时，需要把索引全部改成一样的，当然\n",
    "#      这种情况几乎不存在，也可以暂时忽略）\n",
    "# 解决方法：\n",
    "# 1. 降低numpy版本\n",
    "# 2. 把数据改成二维的，即（10，）-》（1，10） （改完是否存在回测不完整性，没有详细验证）\n",
    "# 3. 保持最上方所示的数据格式（推荐）\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_SAVE_DIR, 'train_data.csv'))\n",
    "train = train.set_index(train.columns[0]) # 第一列为索引\n",
    "train.index.names = ['']\n",
    "assert train.shape[0] > 1, '数据必须至少包含2行，即2天以上'"
   ],
   "metadata": {
    "id": "mFCP1YEhi6oi",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 默认定义了8个技术因子\n",
    "INDICATORS"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwk32SeKJGWZ",
    "outputId": "258b8796-6fd7-445e-eb4a-d964597d248b",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['macd',\n 'boll_ub',\n 'boll_lb',\n 'rsi_30',\n 'cci_30',\n 'dx_30',\n 'close_30_sma',\n 'close_60_sma']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 共29支股票，状态空间291\n",
    "stock_dimension = len(train.tic.unique())\n",
    "# 状态说明\n",
    "# 1：账户余额\n",
    "# [1: stock_dimension+1]: 股票价格\n",
    "# [stock_dimension+1: 1 + 2*stock_dimension]: 持仓数量\n",
    "# len(INDICATORS)*stock_dimension：每支股票的因子状态，bool型表示\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "1455279f-d280-4a4f-a555-935fadd2bdb7",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension  # 手续费\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ],
   "metadata": {
    "id": "WsOLoeNcJF8Q",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 构建训练环境"
   ],
   "metadata": {
    "id": "7We-q73jjaFQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "35605c17-bdda-4f30-86bd-6db9b80c1f1e",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# 3. 训练深度强化学习模型\n",
    "* 强化学习库：使用 **Stable Baselines 3**. 也可以尝试更换 **ElegantRL** and **Ray RLlib**.\n",
    "* FinRL库包含精调的标准深度强化学习算法, 包括DQN, DDPG, Multi-Agent DDPG, PPO, SAC, A2C and TD3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "364PsqckttcQ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 选择需要使用的强化学习算法\n",
    "\n",
    "# 5种算法：A2C, DDPG, PPO, TD3, SAC\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = False\n",
    "if_using_td3 = False\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "## Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GUCnkn-HIbmj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "af2f8c1e-a300-4dc4-fe3d-84861bb4606d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to results/a2c\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.0406    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -36.1      |\n",
      "|    reward             | 0.35875356 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.791      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -78.3      |\n",
      "|    reward             | -1.6108295 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.61       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -64.2    |\n",
      "|    reward             | 3.631484 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.39     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.0566   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 0.681     |\n",
      "|    reward             | 1.8916856 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.53      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 749        |\n",
      "|    reward             | -11.120101 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 405        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 39          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.252      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 221         |\n",
      "|    reward             | -0.13628662 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 29.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -118      |\n",
      "|    reward             | -2.508693 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.32      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 108        |\n",
      "|    reward             | -1.7527727 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.0701     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 326        |\n",
      "|    reward             | -2.9040139 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 76.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -51        |\n",
      "|    reward             | -2.9847684 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -407      |\n",
      "|    reward             | 10.155772 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 95.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.123     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -106       |\n",
      "|    reward             | 0.21866067 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.67       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 87        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 40.5      |\n",
      "|    reward             | -4.783373 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 14.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 94         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 102        |\n",
      "|    reward             | 0.87015337 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 281       |\n",
      "|    reward             | 1.6805226 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 49.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 145       |\n",
      "|    reward             | 2.9370952 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 16.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 115      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -314     |\n",
      "|    reward             | 4.074719 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 96.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -95.6     |\n",
      "|    reward             | 1.1238396 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.47      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -7.63e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -41.4     |\n",
      "|    reward             | 0.2509082 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.45      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -21.4      |\n",
      "|    reward             | 0.62828004 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.83       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 145      |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 54.7     |\n",
      "|    reward             | 1.872283 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.26     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 151        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 35.5       |\n",
      "|    reward             | -6.1961203 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.84       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 158        |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -1.87e+03  |\n",
      "|    reward             | -1.7609289 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.31e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.109    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 29.4      |\n",
      "|    reward             | 0.9932138 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 172        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -36.8      |\n",
      "|    reward             | -1.0857874 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 30.8      |\n",
      "|    reward             | 1.9476856 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.09      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 186         |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | -29.3       |\n",
      "|    reward             | -0.87417066 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.91        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 193       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 359       |\n",
      "|    reward             | 4.5537124 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 102       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 200       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -99.4     |\n",
      "|    reward             | 1.5243515 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 207        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | -16.4      |\n",
      "|    reward             | -0.5839151 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.316      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 214         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -0.376      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 126         |\n",
      "|    reward             | -0.48659432 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 11.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 221       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 8.96      |\n",
      "|    reward             | -2.616014 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 18.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 227         |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | -21.7       |\n",
      "|    reward             | -0.08677213 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.73        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 234       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -34       |\n",
      "|    reward             | 8.089299  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.93      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 241        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 149        |\n",
      "|    reward             | 0.35470638 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 19         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 248       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -60.8     |\n",
      "|    reward             | 1.1239778 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.53      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 257        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 205        |\n",
      "|    reward             | 0.48268092 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 28.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 266       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 41.3      |\n",
      "|    reward             | 2.9179494 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.34      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 273       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -210      |\n",
      "|    reward             | 1.9574504 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 27.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 280       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -202      |\n",
      "|    reward             | 5.6917534 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 41.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 287        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.016     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 15.2       |\n",
      "|    reward             | -0.2988192 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.805      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 294        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -79.3      |\n",
      "|    reward             | 0.35317314 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.96       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 300       |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -62.7     |\n",
      "|    reward             | 3.6175206 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.53      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 307       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 103       |\n",
      "|    reward             | 1.9632988 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 12        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 314       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 300       |\n",
      "|    reward             | 1.0810221 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 64.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 321        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.0056    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 176        |\n",
      "|    reward             | -0.5340171 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 26         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 328        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -141       |\n",
      "|    reward             | 0.37438822 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 14         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 335       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.00435  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -178      |\n",
      "|    reward             | -0.686751 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 19        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 342       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -35.1     |\n",
      "|    reward             | 2.2667644 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.62      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 349       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -34.1     |\n",
      "|    reward             | 1.7236179 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.44      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 356        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 344        |\n",
      "|    reward             | -1.3115964 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 92.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 363       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -68.5     |\n",
      "|    reward             | 10.192349 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 255       |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6948725.78\n",
      "total_reward: 5948725.78\n",
      "total_cost: 20825.46\n",
      "total_trades: 52275\n",
      "Sharpe: 0.982\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 370        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | -34.4      |\n",
      "|    reward             | 0.86311513 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.765      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 377        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -24.5      |\n",
      "|    reward             | 0.18445487 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.2        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 384       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 122       |\n",
      "|    reward             | 2.8554468 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 393      |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -135     |\n",
      "|    reward             | 2.559949 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 12       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 399        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -501       |\n",
      "|    reward             | -5.3341117 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 159        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 406       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.016    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -30.6     |\n",
      "|    reward             | 1.4052856 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 413        |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 36.5       |\n",
      "|    reward             | -0.3621401 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 6000        |\n",
      "|    time_elapsed       | 419         |\n",
      "|    total_timesteps    | 30000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5999        |\n",
      "|    policy_loss        | 116         |\n",
      "|    reward             | -0.50633395 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 8.91        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 425        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | -1.96      |\n",
      "|    reward             | -2.2409592 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.95       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 431         |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | 71.5        |\n",
      "|    reward             | 0.005055887 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.38        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 437       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 430       |\n",
      "|    reward             | 7.5756755 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 179       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 443      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 27.4     |\n",
      "|    reward             | 0.868511 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.632    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 450        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 34.3       |\n",
      "|    reward             | -2.7578495 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.81       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 456       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | -77.8     |\n",
      "|    reward             | 0.3686749 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.46      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 462      |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -820     |\n",
      "|    reward             | -7.68705 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 450      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 469       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -65.2     |\n",
      "|    reward             | 1.3140092 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.99      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 475        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -344       |\n",
      "|    reward             | -1.2876214 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 124        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 482       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 38.6      |\n",
      "|    reward             | 0.1618151 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.09      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 488       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 66.9      |\n",
      "|    reward             | 0.4717599 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.45      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 494       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -213      |\n",
      "|    reward             | 1.1371102 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 30.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 500        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -43.2      |\n",
      "|    reward             | -0.5139027 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.31       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 506       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 190       |\n",
      "|    reward             | -4.752103 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 31.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 512       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 611       |\n",
      "|    reward             | -7.086766 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 229       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 518        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | -132       |\n",
      "|    reward             | 0.77915365 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 10.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 524       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -92.1     |\n",
      "|    reward             | 1.3087561 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.31      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 530      |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | 4        |\n",
      "|    reward             | 0.617661 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.64     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 536      |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 2.57     |\n",
      "|    reward             | 5.718696 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 27       |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 543       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -169      |\n",
      "|    reward             | -2.240522 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 30.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 548       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 60.1      |\n",
      "|    reward             | 8.026444  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 555        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -218       |\n",
      "|    reward             | 0.23244627 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 28.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 561        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 19.9       |\n",
      "|    reward             | -2.0067334 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.425      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 567        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | 47.6       |\n",
      "|    reward             | -1.1969955 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.14       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 573         |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -459        |\n",
      "|    reward             | -0.08864777 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 126         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 579        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 207        |\n",
      "|    reward             | -21.636715 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 50.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 585        |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0.00099    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 3.57       |\n",
      "|    reward             | 0.24098516 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.971      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 591       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -46.3     |\n",
      "|    reward             | 1.0104673 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 597       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 170       |\n",
      "|    reward             | 3.4449139 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 19.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 603        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 148        |\n",
      "|    reward             | -0.7493182 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 22.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 609       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0.000565  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -64       |\n",
      "|    reward             | 2.0271256 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.3       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 615      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | 20.6     |\n",
      "|    reward             | 5.208913 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 12.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 621        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | -103       |\n",
      "|    reward             | 0.90547246 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.73       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 627        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 168        |\n",
      "|    reward             | -0.4858205 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 16.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 633       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 117       |\n",
      "|    reward             | 0.3112922 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 12.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 639         |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | -135        |\n",
      "|    reward             | -0.65156066 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 12.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 646        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 97.6       |\n",
      "|    reward             | -0.9942001 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 7.71       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 652       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 193       |\n",
      "|    reward             | 7.3135386 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 49.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 658        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | 33.7       |\n",
      "|    reward             | 0.58441216 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.648      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 75            |\n",
      "|    iterations         | 10000         |\n",
      "|    time_elapsed       | 664           |\n",
      "|    total_timesteps    | 50000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.8         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9999          |\n",
      "|    policy_loss        | 50.9          |\n",
      "|    reward             | 0.00045472736 |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 2.63          |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10100      |\n",
      "|    time_elapsed       | 670        |\n",
      "|    total_timesteps    | 50500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10099      |\n",
      "|    policy_loss        | -10.9      |\n",
      "|    reward             | 0.41431746 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10200      |\n",
      "|    time_elapsed       | 677        |\n",
      "|    total_timesteps    | 51000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10199      |\n",
      "|    policy_loss        | 217        |\n",
      "|    reward             | -0.2106421 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 25.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10300      |\n",
      "|    time_elapsed       | 683        |\n",
      "|    total_timesteps    | 51500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10299      |\n",
      "|    policy_loss        | -203       |\n",
      "|    reward             | -2.2708597 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 33.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 10400     |\n",
      "|    time_elapsed       | 690       |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10399     |\n",
      "|    policy_loss        | -269      |\n",
      "|    reward             | 17.584393 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 471       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 696        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | 94.1       |\n",
      "|    reward             | 0.66290647 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.13       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 702        |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | 8.49       |\n",
      "|    reward             | -1.0744202 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.193      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 10700     |\n",
      "|    time_elapsed       | 709       |\n",
      "|    total_timesteps    | 53500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10699     |\n",
      "|    policy_loss        | 22.2      |\n",
      "|    reward             | 0.7662854 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 715       |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -65.1     |\n",
      "|    reward             | 0.5330719 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.95      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 721        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | -56.3      |\n",
      "|    reward             | 0.14222226 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.92       |\n",
      "--------------------------------------\n",
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4851550.12\n",
      "total_reward: 3851550.12\n",
      "total_cost: 5764.62\n",
      "total_trades: 39011\n",
      "Sharpe: 0.894\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 727        |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | -14.8      |\n",
      "|    reward             | -1.0496396 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 11100       |\n",
      "|    time_elapsed       | 734         |\n",
      "|    total_timesteps    | 55500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11099       |\n",
      "|    policy_loss        | 59.7        |\n",
      "|    reward             | -0.16290869 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.68        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 11200       |\n",
      "|    time_elapsed       | 740         |\n",
      "|    total_timesteps    | 56000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11199       |\n",
      "|    policy_loss        | -16.9       |\n",
      "|    reward             | -0.15370224 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.563       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 11300     |\n",
      "|    time_elapsed       | 746       |\n",
      "|    total_timesteps    | 56500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11299     |\n",
      "|    policy_loss        | -126      |\n",
      "|    reward             | 3.7041051 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 21.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 11400      |\n",
      "|    time_elapsed       | 753        |\n",
      "|    total_timesteps    | 57000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11399      |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | -0.5704795 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 7.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 11500      |\n",
      "|    time_elapsed       | 760        |\n",
      "|    total_timesteps    | 57500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11499      |\n",
      "|    policy_loss        | 195        |\n",
      "|    reward             | -0.4250386 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 21.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 766        |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | 146        |\n",
      "|    reward             | 0.71528834 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 19.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 772        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | -70.8      |\n",
      "|    reward             | 0.28594398 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.13       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 11800      |\n",
      "|    time_elapsed       | 779        |\n",
      "|    total_timesteps    | 59000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11799      |\n",
      "|    policy_loss        | 83         |\n",
      "|    reward             | -0.4554322 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 5.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 11900     |\n",
      "|    time_elapsed       | 785       |\n",
      "|    total_timesteps    | 59500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11899     |\n",
      "|    policy_loss        | 14.8      |\n",
      "|    reward             | 2.7885983 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 5.59      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 791        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -43        |\n",
      "|    reward             | 0.47410363 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.06       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 798       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | 149       |\n",
      "|    reward             | 2.4825547 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 22.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 12200      |\n",
      "|    time_elapsed       | 804        |\n",
      "|    total_timesteps    | 61000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12199      |\n",
      "|    policy_loss        | -59.4      |\n",
      "|    reward             | 0.28152287 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 12300      |\n",
      "|    time_elapsed       | 810        |\n",
      "|    total_timesteps    | 61500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12299      |\n",
      "|    policy_loss        | -66.2      |\n",
      "|    reward             | -1.5573747 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 12400      |\n",
      "|    time_elapsed       | 816        |\n",
      "|    total_timesteps    | 62000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12399      |\n",
      "|    policy_loss        | 45.4       |\n",
      "|    reward             | 0.47542864 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 12500      |\n",
      "|    time_elapsed       | 822        |\n",
      "|    total_timesteps    | 62500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12499      |\n",
      "|    policy_loss        | -189       |\n",
      "|    reward             | 0.94777614 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 18.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 829        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | 247        |\n",
      "|    reward             | 0.37721524 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 32.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 12700      |\n",
      "|    time_elapsed       | 836        |\n",
      "|    total_timesteps    | 63500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12699      |\n",
      "|    policy_loss        | 142        |\n",
      "|    reward             | -2.3181558 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 12800       |\n",
      "|    time_elapsed       | 842         |\n",
      "|    total_timesteps    | 64000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12799       |\n",
      "|    policy_loss        | -151        |\n",
      "|    reward             | 0.041497517 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 18.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 12900      |\n",
      "|    time_elapsed       | 848        |\n",
      "|    total_timesteps    | 64500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12899      |\n",
      "|    policy_loss        | -66        |\n",
      "|    reward             | -0.6765446 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.6        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 854       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | -45.8     |\n",
      "|    reward             | 2.2144587 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.17      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 861      |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | 37.3     |\n",
      "|    reward             | 1.642958 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 1.59     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13200     |\n",
      "|    time_elapsed       | 867       |\n",
      "|    total_timesteps    | 66000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13199     |\n",
      "|    policy_loss        | -264      |\n",
      "|    reward             | 3.6487794 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 47.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 873       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | 586       |\n",
      "|    reward             | -4.994499 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 203       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 880         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 2.38e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | 7.24        |\n",
      "|    reward             | -0.12977643 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.52        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 13500       |\n",
      "|    time_elapsed       | 886         |\n",
      "|    total_timesteps    | 67500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13499       |\n",
      "|    policy_loss        | 71.4        |\n",
      "|    reward             | -0.27040747 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 4.18        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 892        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -384       |\n",
      "|    reward             | -1.6518041 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 92         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 13700       |\n",
      "|    time_elapsed       | 898         |\n",
      "|    total_timesteps    | 68500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13699       |\n",
      "|    policy_loss        | -126        |\n",
      "|    reward             | -0.92655075 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 12.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13800      |\n",
      "|    time_elapsed       | 904        |\n",
      "|    total_timesteps    | 69000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13799      |\n",
      "|    policy_loss        | -141       |\n",
      "|    reward             | -7.0051146 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 11.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 910        |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0.00129    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 149        |\n",
      "|    reward             | 0.24578345 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 15.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 917        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | 9.12       |\n",
      "|    reward             | 0.07170665 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.487      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 923        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | 173        |\n",
      "|    reward             | -2.3600714 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 18         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 14200       |\n",
      "|    time_elapsed       | 930         |\n",
      "|    total_timesteps    | 71000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | -2.38e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14199       |\n",
      "|    policy_loss        | 183         |\n",
      "|    reward             | -0.17162097 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 20.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 936       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 17.6      |\n",
      "|    reward             | 0.9420935 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.841     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 942        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | 291        |\n",
      "|    reward             | -3.6963248 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 50.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 14500       |\n",
      "|    time_elapsed       | 948         |\n",
      "|    total_timesteps    | 72500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0.444       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14499       |\n",
      "|    policy_loss        | -26         |\n",
      "|    reward             | -0.30802673 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.848       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 955       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | 49.3      |\n",
      "|    reward             | 2.8035235 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 961       |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | -202      |\n",
      "|    reward             | 0.9818608 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 22        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 967        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | -435       |\n",
      "|    reward             | -1.2765247 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 105        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 974        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | 158        |\n",
      "|    reward             | -2.7518125 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 16.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 980       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | 200       |\n",
      "|    reward             | -5.519703 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 31        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 986       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | -58.1     |\n",
      "|    reward             | 0.3461819 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 5.52      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15200      |\n",
      "|    time_elapsed       | 992        |\n",
      "|    total_timesteps    | 76000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15199      |\n",
      "|    policy_loss        | -7.02      |\n",
      "|    reward             | 0.70825624 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.326      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 999       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | -85.5     |\n",
      "|    reward             | 4.257844  |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 12.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 1005      |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | 219       |\n",
      "|    reward             | -3.102746 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 27.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 1011      |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | -0.00131  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 12.2      |\n",
      "|    reward             | 1.9700869 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 3.89      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 1018       |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 226        |\n",
      "|    reward             | -1.0513586 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 37.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15700      |\n",
      "|    time_elapsed       | 1024       |\n",
      "|    total_timesteps    | 78500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | -4.77e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15699      |\n",
      "|    policy_loss        | 93.6       |\n",
      "|    reward             | -2.1067793 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 8.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 1030      |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 6.66      |\n",
      "|    reward             | 3.3519413 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 6.6       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 15900      |\n",
      "|    time_elapsed       | 1036       |\n",
      "|    total_timesteps    | 79500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15899      |\n",
      "|    policy_loss        | 137        |\n",
      "|    reward             | 0.31606784 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 16000    |\n",
      "|    time_elapsed       | 1042     |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -332     |\n",
      "|    reward             | 1.257614 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 96.3     |\n",
      "------------------------------------\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "D:\\ProgramFiles\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:276: UserWarning: Path 'trained_models\\a2c' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "if if_using_a2c:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/a2c'\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_a2c.set_logger(new_logger_a2c)\n",
    "    \n",
    "    trained_a2c = agent.train_model(model=model_a2c, \n",
    "                                 tb_log_name='a2c',\n",
    "                                 total_timesteps=80000)\n",
    "\n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'a2c') \n",
    "    trained_a2c.save(os.path.join(save_path, \"agent_a2c.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "## Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M2YadjfnLwgt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90405544-f219-431a-8fe4-76bebba1be54",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "if if_using_ddpg:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/ddpg'\n",
    "    new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_ddpg.set_logger(new_logger_ddpg)\n",
    "    \n",
    "    trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) \n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'ddpg') \n",
    "    trained_ddpg.save(os.path.join(save_path, \"agent_ddpg.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y5D5PFUhMzSV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f378334f-833f-4ec1-e2ba-f12a0bdebf70",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# PPO_PARAMS = {\n",
    "#     \"n_steps\": 2048,\n",
    "#     \"ent_coef\": 0.01,\n",
    "#     \"learning_rate\": 0.00025,\n",
    "#     \"batch_size\": 128,\n",
    "# }\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 512,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "if if_using_ppo:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/ppo'\n",
    "    new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_ppo.set_logger(new_logger_ppo)\n",
    "    \n",
    "    trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000)\n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'ppo') \n",
    "    trained_ppo.save(os.path.join(save_path, \"agent_ppo.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JSAHhV4Xc-bh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "283fa6a7-1997-4816-8582-a9fcdcfa73d0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "\n",
    "if if_using_td3:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/td3'\n",
    "    new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_td3.set_logger(new_logger_td3)\n",
    "    \n",
    "    trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000)\n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'td3') \n",
    "    trained_td3.save(os.path.join(save_path, \"agent_td3\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwOhVjqRkCdM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f7688355-6ae8-483c-ec33-d202bcab702e",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to results\\sac\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4820714.00\n",
      "total_reward: 3820714.00\n",
      "total_cost: 155147.33\n",
      "total_trades: 54115\n",
      "Sharpe: 0.838\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 368       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 432       |\n",
      "|    critic_loss     | 106       |\n",
      "|    ent_coef        | 0.11      |\n",
      "|    ent_coef_loss   | -88.7     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 2.8252904 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 737      |\n",
      "|    total_timesteps | 23144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 124      |\n",
      "|    critic_loss     | 40.7     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | -117     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 23043    |\n",
      "|    reward          | 5.568931 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4591542.00\n",
      "total_reward: 3591542.00\n",
      "total_cost: 6405.34\n",
      "total_trades: 46623\n",
      "Sharpe: 0.769\n",
      "=================================\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 在4G单卡GTX 1050 Ti GPU下, batch调大显存占用和利用率也上不去，不确定原因\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "if if_using_sac:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = os.path.join(RESULTS_DIR, 'sac') \n",
    "    new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_sac.set_logger(new_logger_sac)\n",
    "    \n",
    "    trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000)\n",
    "\n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'sac') \n",
    "    trained_sac.save(os.path.join(save_path, 'agent_sac.zip'))"
   ]
  }
 ]
}