{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "uijiWgkuh1jB",
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ]
  },
  "kernelspec": {
   "name": "pycharm-3ea5732f",
   "language": "python",
   "display_name": "PyCharm (ai_quant_trade)"
  },
  "language_info": {
   "name": "python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 安装依赖包，导入头文件"
   ],
   "metadata": {
    "id": "gT-zXutMgqOS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 如果没有安装，解注释进行安装\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "## 请把下面解注释，安装finrl库\n",
    "# 或者把如下Github仓中的finrl文件夹考到根目录即可使用\n",
    "##!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
    "\n",
    "# 强化学习库，使用stable_baselines3\n",
    "# 注意：\n",
    "#    1. 强化学习比起机器学习慢很多，CPU训练大约2分钟，强化学习使用单卡GPU大约需要30分钟左右完成训练\n",
    "#    2. 强化学习不稳定，每次收敛的loss不一样，且效果可能差异大     "
   ],
   "metadata": {
    "id": "D0vEcPxSJ8hI",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.meta.data_processor import DataProcessor"
   ],
   "metadata": {
    "id": "xt1317y2ixSS",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from finrl import config  # 包含各类超参\n",
    "from finrl import config_tickers  # 常见各类市场股票代码集合，比如中证300\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "# 创建目录\n",
    "check_and_make_directories([TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ],
   "metadata": {
    "id": "wZ7Bl7i6I2AM",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 在OpenAI Gym-style构建市场环境"
   ],
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 读取训练数据，所有股票均混在了一个csv表里，格式如下\n",
    "# 索引     日期          股票\n",
    "#  0      2009-01-02    苹果\n",
    "#  0      2009-01-02    亚马逊\n",
    "#  1      2009-01-05    苹果\n",
    "#  1      2009-01-05    亚马逊\n",
    "\n",
    "# 注意：必须保持上述该格式，同样的索引下至少有2个数据，否则会报错，\n",
    "# 原因：\n",
    "#   1. 在finrl/meta/env_stock_trading/env_stocktrading.py的\n",
    "#      _initiate_state函数中self.data.close.values.tolist()，\n",
    "#      在404行，要求self.data.close必须是二维数组\n",
    "#   2. 而finrl/meta/env_stock_trading/env_stocktrading.py的\n",
    "#      __init__的64行self.data = self.df.loc[self.day, :]，\n",
    "#      如果索引顺序排，0，1，2。。。，会导致只取到一个行数，一维\n",
    "#      数据传入导致第1点中所述的问题\n",
    "#      （因此，如果只有一支股票时，需要把索引全部改成一样的，当然\n",
    "#      这种情况几乎不存在，也可以暂时忽略）\n",
    "# 解决方法：\n",
    "# 1. 降低numpy版本\n",
    "# 2. 把数据改成二维的，即（10，）-》（1，10） （改完是否存在回测不完整性，没有详细验证）\n",
    "# 3. 保持最上方所示的数据格式（推荐）\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_SAVE_DIR, 'train_data.csv'))\n",
    "train = train.set_index(train.columns[0]) # 第一列为索引\n",
    "train.index.names = ['']\n",
    "assert train.shape[0] > 1, '数据必须至少包含2行，即2天以上'"
   ],
   "metadata": {
    "id": "mFCP1YEhi6oi",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "D:\\Program Files\\JetBrains\\PyCharm 2019.3.3\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_utils.py:462: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in s.iteritems():\n",
      "D:\\Program Files\\JetBrains\\PyCharm 2019.3.3\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_utils.py:462: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in s.iteritems():\n",
      "D:\\Program Files\\JetBrains\\PyCharm 2019.3.3\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_utils.py:462: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in s.iteritems():\n",
      "D:\\Program Files\\JetBrains\\PyCharm 2019.3.3\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_utils.py:462: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in s.iteritems():\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 默认定义了8个技术因子\n",
    "INDICATORS"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwk32SeKJGWZ",
    "outputId": "258b8796-6fd7-445e-eb4a-d964597d248b",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['macd',\n 'boll_ub',\n 'boll_lb',\n 'rsi_30',\n 'cci_30',\n 'dx_30',\n 'close_30_sma',\n 'close_60_sma']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 共29支股票，状态空间291\n",
    "stock_dimension = len(train.tic.unique())\n",
    "# 状态说明\n",
    "# 1：账户余额\n",
    "# [1: stock_dimension+1]: 股票价格\n",
    "# [stock_dimension+1: 1 + 2*stock_dimension]: 持仓数量\n",
    "# len(INDICATORS)*stock_dimension：每支股票的因子状态，bool型表示\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "1455279f-d280-4a4f-a555-935fadd2bdb7",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension  # 手续费\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ],
   "metadata": {
    "id": "WsOLoeNcJF8Q",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 构建训练环境"
   ],
   "metadata": {
    "id": "7We-q73jjaFQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "35605c17-bdda-4f30-86bd-6db9b80c1f1e",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# 3. 训练深度强化学习模型\n",
    "* 强化学习库：使用 **Stable Baselines 3**. 也可以尝试更换 **ElegantRL** and **Ray RLlib**.\n",
    "* FinRL库包含精调的标准深度强化学习算法, 包括DQN, DDPG, Multi-Agent DDPG, PPO, SAC, A2C and TD3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "364PsqckttcQ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 选择需要使用的强化学习算法\n",
    "\n",
    "# 5种算法：A2C, DDPG, PPO, TD3, SAC\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = False\n",
    "if_using_td3 = False\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "## Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GUCnkn-HIbmj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "af2f8c1e-a300-4dc4-fe3d-84861bb4606d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if if_using_a2c:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/a2c'\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_a2c.set_logger(new_logger_a2c)\n",
    "    \n",
    "    trained_a2c = agent.train_model(model=model_a2c, \n",
    "                                 tb_log_name='a2c',\n",
    "                                 total_timesteps=80000)\n",
    "\n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'a2c') \n",
    "    trained_a2c.save(os.path.join(save_path, \"agent_a2c.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "## Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M2YadjfnLwgt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90405544-f219-431a-8fe4-76bebba1be54",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "if if_using_ddpg:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/ddpg'\n",
    "    new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_ddpg.set_logger(new_logger_ddpg)\n",
    "    \n",
    "    trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) \n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'ddpg') \n",
    "    trained_ddpg.save(os.path.join(save_path, \"agent_ddpg.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y5D5PFUhMzSV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f378334f-833f-4ec1-e2ba-f12a0bdebf70",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# PPO_PARAMS = {\n",
    "#     \"n_steps\": 2048,\n",
    "#     \"ent_coef\": 0.01,\n",
    "#     \"learning_rate\": 0.00025,\n",
    "#     \"batch_size\": 128,\n",
    "# }\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 512,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "if if_using_ppo:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/ppo'\n",
    "    new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_ppo.set_logger(new_logger_ppo)\n",
    "    \n",
    "    trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000)\n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'ppo') \n",
    "    trained_ppo.save(os.path.join(save_path, \"agent_ppo.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JSAHhV4Xc-bh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "283fa6a7-1997-4816-8582-a9fcdcfa73d0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "\n",
    "if if_using_td3:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/td3'\n",
    "    new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_td3.set_logger(new_logger_td3)\n",
    "    \n",
    "    trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000)\n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'td3') \n",
    "    trained_td3.save(os.path.join(save_path, \"agent_td3\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xwOhVjqRkCdM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f7688355-6ae8-483c-ec33-d202bcab702e",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to results\\sac\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 358       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.08e+03  |\n",
      "|    critic_loss     | 81.2      |\n",
      "|    ent_coef        | 0.184     |\n",
      "|    ent_coef_loss   | -80.7     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 2.1083984 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 717       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 555       |\n",
      "|    critic_loss     | 31.8      |\n",
      "|    ent_coef        | 0.0596    |\n",
      "|    ent_coef_loss   | -112      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 11.106202 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9932360.60\n",
      "total_reward: 8932360.60\n",
      "total_cost: 12393.65\n",
      "total_trades: 45915\n",
      "Sharpe: 1.106\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 33         |\n",
      "|    time_elapsed    | 1050       |\n",
      "|    total_timesteps | 34716      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 332        |\n",
      "|    critic_loss     | 88.6       |\n",
      "|    ent_coef        | 0.0191     |\n",
      "|    ent_coef_loss   | -127       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 34615      |\n",
      "|    reward          | 13.4701605 |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 1391      |\n",
      "|    total_timesteps | 46288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 190       |\n",
      "|    critic_loss     | 51.5      |\n",
      "|    ent_coef        | 0.00635   |\n",
      "|    ent_coef_loss   | -92.7     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 46187     |\n",
      "|    reward          | 10.458529 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5724780.24\n",
      "total_reward: 4724780.24\n",
      "total_cost: 2187.43\n",
      "total_trades: 49010\n",
      "Sharpe: 0.892\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 1731      |\n",
      "|    total_timesteps | 57860     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 115       |\n",
      "|    critic_loss     | 21.4      |\n",
      "|    ent_coef        | 0.00262   |\n",
      "|    ent_coef_loss   | -6.23     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 57759     |\n",
      "|    reward          | 2.2610304 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 2070      |\n",
      "|    total_timesteps | 69432     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 72.9      |\n",
      "|    critic_loss     | 10.5      |\n",
      "|    ent_coef        | 0.00276   |\n",
      "|    ent_coef_loss   | -0.736    |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 69331     |\n",
      "|    reward          | 1.4454446 |\n",
      "----------------------------------\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 在4G单卡GTX 1050 Ti GPU下, batch调大显存占用和利用率也上不去，不确定原因\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "if if_using_sac:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = os.path.join(RESULTS_DIR, 'sac') \n",
    "    new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_sac.set_logger(new_logger_sac)\n",
    "    \n",
    "    trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000)\n",
    "\n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'sac') \n",
    "    trained_sac.save(os.path.join(save_path, 'agent_sac.zip'))"
   ]
  }
 ]
}